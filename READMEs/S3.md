## Setting up S3 Bucket(s)

There are only two buckets, and they have already been created and provisioned on S3. One bucket for use by development machines and one for production. Each machine, dev or prod, has a ~/.aws directory containing two files: config and credentials, already set up. config and credentials together identify the AWS region that holds our bucket(s) and aws_access_key and aws_secret_access_key. Taken in conjunction with AWS_BUCKET_NAME from program .env files that provides enough that SDK calls can get to the correct place.

These are the steps that were taken to set up the two buckets:

1. Let BASE_NAME = 'abc'.
2. AWS console
3. S3: create new bucket; no other settings; Create Bucket.
4. IAM Users
5. Create user
6. BASE_NAME = "xxxAdmin"; Create User
7. Click on user name
8. Click on Groups; click Add user to groups button
9. Create new group with AdministratorAccess policy; call it BASE_NAME_Group
10. Click check box next to Group just created; click Add user to group(s) button
11. Click to open Security credentials tab; click Create access key button
12. Click Application running outside AWS radio button; click Next button
13. Name of new key: "BASE_NAME_key"; click Next button
14. On Retrieve access keys you can see Access key and masked Secret access key; copy both someplace
15. Click Done
16. Add to appropriate .ini file

## S3 steps to do
- ~~Upload directly to S3, creating very incomplete record in results table.~~
- Since /surrogate is gone, as is /processed, trigger transcription and scoring by launching a recurring select from results that looks for these incompletes, processes each of them and updates their results record.
    - Although we want this task to recur fairly often, make sure prior task's process is completed before launching again.
    - How often? Continuously during the day; every 15 minutes after hours.
    - Processing:
        - start Bree job on recurring schedule
        - query results to find all new (incomplete) records
        - for each record, read audio file from S3; call Deepgram for transcription; score it; run query to update record
- While testing and fixing problems Kent found in Explore Scoring, change audioplayer to fetch from S3.
- Breejob (nightly email to org admins sender) may not have to change at all, but it does still need to construct the hyperlink (if that's possible).
- Movetocool is most likely unnecessary and it can be deleted; it was meant to move from /processed to S3.
